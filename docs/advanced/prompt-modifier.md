# PromptModifier API å‚è€ƒ

`PromptModifier` æä¾›å¯¹åº•å±‚ LLM çš„ç›´æ¥è®¿é—®ï¼Œå®Œå…¨ç»•è¿‡éº¦éº¦çš„äººæ ¼/ä¸Šä¸‹æ–‡å±‚ï¼Œé€‚ç”¨äºéœ€è¦ç²¾ç¡®æ§åˆ¶æç¤ºè¯çš„é«˜çº§åœºæ™¯ã€‚

## å¯¼å…¥

```python
from mai_advanced import PromptModifier
```

## å®ä¾‹åŒ–

```python
async def execute(self):
    modifier = PromptModifier(self)  # ä¼ å…¥ self
    ...
```

---

## æ–¹æ³•åˆ—è¡¨

### `get_available_models()`

è·å–ç³»ç»Ÿé…ç½®çš„æ‰€æœ‰å¯ç”¨ LLM æ¨¡å‹ã€‚

```python
def get_available_models(self) -> Dict[str, TaskConfig]
```

**è¿”å›ï¼š** `{æ¨¡å‹å: TaskConfig}` å­—å…¸

**ç¤ºä¾‹ï¼š**
```python
models = modifier.get_available_models()
for name in models.keys():
    print(name)  # å¦‚ "gpt-4o", "deepseek-v3"
```

---

### `get_default_model()`

è·å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„é»˜è®¤æ¨¡å‹é…ç½®ã€‚

```python
def get_default_model(self) -> Optional[TaskConfig]
```

---

### `call_model()`

ç›´æ¥è°ƒç”¨æŒ‡å®š LLM æ¨¡å‹ï¼Œå®Œå…¨æ§åˆ¶æç¤ºè¯å†…å®¹ã€‚

```python
async def call_model(
    self,
    prompt: str,
    model_name: Optional[str] = None,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    request_type: str = "plugin.advanced",
) -> Tuple[bool, str]
```

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `prompt` | `str` | å®Œæ•´æç¤ºè¯ |
| `model_name` | `str` | æ¨¡å‹åç§°ï¼ˆ`None` åˆ™ç”¨é»˜è®¤æ¨¡å‹ï¼‰ |
| `temperature` | `float` | ç”Ÿæˆæ¸©åº¦ï¼ˆ0.0~1.0ï¼Œè¶Šå°è¶Šç¡®å®šï¼‰ |
| `max_tokens` | `int` | æœ€å¤§è¾“å‡º token æ•° |
| `request_type` | `str` | æ—¥å¿—æ ‡è¯†ç¬¦ |

**è¿”å›ï¼š** `(success: bool, generated_text: str)`

**åº•å±‚ APIï¼š** `llm_api.generate_with_model()`

**ç¤ºä¾‹ï¼š**
```python
# ç”Ÿæˆ JSON
ok, result = await modifier.call_model(
    prompt='è¯·è¿”å› JSONï¼š{"city": "åŒ—äº¬", "weather": "æ™´", "temp": 25}',
    temperature=0.0,   # å®Œå…¨ç¡®å®šæ€§è¾“å‡º
    max_tokens=200,
)
if ok:
    import json
    data = json.loads(result)

# ä½¿ç”¨ç‰¹å®šæ¨¡å‹
ok, code = await modifier.call_model(
    prompt="ç”¨ Python å†™ä¸€ä¸ªå†’æ³¡æ’åº",
    model_name="deepseek-v3",
    temperature=0.2,
)
```

---

### `call_model_with_tools()`

ä½¿ç”¨å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰æ¨¡å¼è°ƒç”¨ LLMã€‚

```python
async def call_model_with_tools(
    self,
    prompt: str,
    tool_names: Optional[List[str]] = None,
    model_name: Optional[str] = None,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> Tuple[bool, str, Optional[List[ToolCall]]]
```

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `prompt` | `str` | æç¤ºè¯ |
| `tool_names` | `List[str]` | è¦å¯ç”¨çš„å·¥å…·åï¼ˆ`None` åˆ™å¯ç”¨å…¨éƒ¨ï¼‰ |
| `model_name` | `str` | æ¨¡å‹åç§° |
| `temperature` | `float` | ç”Ÿæˆæ¸©åº¦ |
| `max_tokens` | `int` | æœ€å¤§ token æ•° |

**è¿”å›ï¼š** `(success: bool, text_content: str, tool_calls: Optional[List[ToolCall]])`

**åº•å±‚ APIï¼š** `llm_api.generate_with_model_with_tools()`

**ç¤ºä¾‹ï¼š**
```python
ok, content, tool_calls = await modifier.call_model_with_tools(
    prompt="å¸®æˆ‘æŸ¥ä¸€ä¸‹ä»Šå¤©åŒ—äº¬çš„å¤©æ°”",
    tool_names=["web_search"],
)
if tool_calls:
    for call in tool_calls:
        print(f"è°ƒç”¨å·¥å…·: {call.name}")
        print(f"å‚æ•°: {call.arguments}")
```

---

### `generate_with_extra_context()`

åœ¨éº¦éº¦é»˜è®¤æç¤ºè¯ä¸­æ³¨å…¥é¢å¤–ä¸Šä¸‹æ–‡ï¼Œè¿”å›åŸå§‹ `reply_set`ï¼ˆä¸è‡ªåŠ¨å‘é€ï¼‰ã€‚

```python
async def generate_with_extra_context(
    self,
    extra_info: str,
    reply_to: str = "",
    enable_tool: bool = False,
    return_prompt: bool = False,
) -> Tuple[bool, List[Tuple[str, Any]], Optional[str]]
```

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `extra_info` | `str` | è¿½åŠ åˆ°æç¤ºè¯æœ«å°¾çš„æ–‡æœ¬ |
| `reply_to` | `str` | å›å¤ç›®æ ‡ |
| `enable_tool` | `bool` | æ˜¯å¦å¯ç”¨å·¥å…· |
| `return_prompt` | `bool` | æ˜¯å¦è¿”å›æç¤ºè¯ |

**è¿”å›ï¼š** `(success, reply_set, prompt_or_none)`

> ğŸ’¡ ä¸ `AdvancedReplyBuilder.generate_reply()` çš„åŒºåˆ«ï¼šæ­¤æ–¹æ³•è¿”å›åŸå§‹ `reply_set`ï¼Œ**ä¸è‡ªåŠ¨å‘é€**ï¼Œé€‚åˆéœ€è¦è¿›ä¸€æ­¥å¤„ç†å›å¤å†…å®¹çš„åœºæ™¯ã€‚

**ç¤ºä¾‹ï¼š**
```python
from src.plugin_system.apis import send_api

ok, reply_set, prompt = await modifier.generate_with_extra_context(
    extra_info="ä»¥ä¸‹æ˜¯èƒŒæ™¯çŸ¥è¯†ï¼š[çŸ¥è¯†å†…å®¹...]\n\nè¯·åŸºäºä»¥ä¸ŠçŸ¥è¯†å›ç­”é—®é¢˜",
    return_prompt=True,
)
if ok:
    # è¿‡æ»¤æ‰è¡¨æƒ…åŒ…ï¼Œåªå‘æ–‡å­—
    for rtype, rcontent in reply_set:
        if rtype == "text":
            await send_api.text_to_stream(rcontent, self.stream_id)
```

---

### `get_actual_prompt()`

è·å–éº¦éº¦ç”Ÿæˆå›å¤æ—¶å®é™…ä½¿ç”¨çš„å®Œæ•´æç¤ºè¯ï¼ˆè°ƒè¯•ç”¨ï¼Œä¸å‘é€æ¶ˆæ¯ï¼‰ã€‚

```python
async def get_actual_prompt(
    self,
    extra_info: str = "",
) -> Optional[str]
```

**è¿”å›ï¼š** æç¤ºè¯å­—ç¬¦ä¸²ï¼Œæˆ– `None`

**ç¤ºä¾‹ï¼š**
```python
prompt = await modifier.get_actual_prompt()
# è®°å½•åˆ°æ—¥å¿—
self.logger.info(f"å½“å‰æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
```

---

## ä½¿ç”¨åœºæ™¯å¯¹æ¯”

| åœºæ™¯ | æ¨èæ–¹æ³• | è¯´æ˜ |
|------|----------|------|
| ä¸“ä¸šå›ç­”ï¼ˆåŒ»ç”Ÿ/å¾‹å¸ˆ/å¨å¸ˆï¼‰ | `generate_custom_reply()` | å®Œå…¨è‡ªå®šä¹‰ï¼Œä¸å¸¦äººæ ¼ |
| æ³¨å…¥å¤–éƒ¨çŸ¥è¯†ï¼ˆæ–°é—»/æ•°æ®åº“ï¼‰ | `generate_with_extra_context()` | ä¿ç•™äººæ ¼ï¼Œè¿½åŠ çŸ¥è¯† |
| è·å– JSON/ä»£ç è¾“å‡º | `call_model(temperature=0.0)` | ç›´æ¥ LLMï¼Œç²¾ç¡®æ§åˆ¶ |
| ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼ˆæœç´¢/è®¡ç®—ï¼‰ | `call_model_with_tools()` | Function Calling |
| è°ƒè¯•å½“å‰æç¤ºè¯å†…å®¹ | `get_actual_prompt()` | åªè¯»ï¼Œä¸å‘é€ |

---

## å®Œæ•´ç¤ºä¾‹ï¼šå¤©æ°”åŠ©æ‰‹ Action

```python
from mai_advanced import PromptModifier
from src.plugin_system.apis import send_api
import json

class WeatherLLMAction(BaseAction):
    action_name = "weather_llm"
    
    async def execute(self) -> Tuple[bool, str]:
        modifier = PromptModifier(self)
        
        city = self.action_data.get("city", "åŒ—äº¬")
        
        # 1. å…ˆç”¨ç»“æ„åŒ–æ¨¡å¼ä» LLM è·å–å¤©æ°”åˆ†æ
        ok, raw_json = await modifier.call_model(
            prompt=f"""åˆ†æä»¥ä¸‹å¤©æ°”æ•°æ®å¹¶è¿”å› JSONï¼ˆä¸è¦ markdownï¼‰ï¼š
åŸå¸‚ï¼š{city}
æ•°æ®ï¼šæ™´ï¼Œ25Â°Cï¼Œä¸œå—é£3çº§ï¼Œæ¹¿åº¦40%

è¿”å›æ ¼å¼ï¼š{{"summary": "ä¸€å¥è¯æ€»ç»“", "advice": "å‡ºè¡Œå»ºè®®", "emoji": "å¤©æ°”è¡¨æƒ…"}}""",
            temperature=0.1,
        )
        
        if not ok:
            await send_api.text_to_stream("å¤©æ°”æŸ¥è¯¢å¤±è´¥", self.stream_id)
            return False, "LLM å¤±è´¥"
        
        try:
            data = json.loads(raw_json)
            message = f"{data['emoji']} {city}å¤©æ°”ï¼š{data['summary']}\nğŸ’¡ {data['advice']}"
        except json.JSONDecodeError:
            message = raw_json  # è§£æå¤±è´¥å°±ç›´æ¥å‘åŸå§‹å†…å®¹
        
        # 2. ç”¨æç¤ºè¯æ³¨å…¥æ–¹å¼è®©éº¦éº¦ç”¨è‡ªå·±çš„è¯­æ°”å‘å‡ºå»
        ok2, reply_set, _ = await modifier.generate_with_extra_context(
            extra_info=f"å¤©æ°”ä¿¡æ¯ï¼š{message}\n\nè¯·ç”¨ä½ çš„è¯­æ°”å‘Šè¯‰ç”¨æˆ·è¿™ä¸ªå¤©æ°”ä¿¡æ¯ã€‚"
        )
        
        if ok2:
            for rtype, rcontent in reply_set:
                if rtype == "text":
                    await send_api.text_to_stream(rcontent, self.stream_id)
        else:
            await send_api.text_to_stream(message, self.stream_id)
        
        return True, f"{city}å¤©æ°”æŸ¥è¯¢å®Œæˆ"
```
