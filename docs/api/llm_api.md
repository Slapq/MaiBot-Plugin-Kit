# ğŸ¤– LLM API

`llm_api` æ¨¡å—æä¾›ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›´æ¥äº¤äº’çš„èƒ½åŠ›ã€‚

## å¯¼å…¥æ–¹å¼

```python
from src.plugin_system import llm_api
```

---

## å‡½æ•°å‚è€ƒ

### `get_available_models()`

è·å–æ‰€æœ‰åœ¨ MaiBot é…ç½®æ–‡ä»¶ä¸­å®šä¹‰çš„å¯ç”¨æ¨¡å‹é…ç½®ã€‚

```python
def get_available_models() -> Dict[str, TaskConfig]
```

**è¿”å›å€¼ï¼š** æ¨¡å‹åç§°åˆ° `TaskConfig` çš„å­—å…¸

**ç¤ºä¾‹ï¼š**

```python
models = llm_api.get_available_models()
for name, config in models.items():
    print(f"æ¨¡å‹: {name}")

# è·å–ç‰¹å®šæ¨¡å‹é…ç½®
models = llm_api.get_available_models()
my_model = models.get("utils")  # ä½¿ç”¨ utils ä»»åŠ¡çš„æ¨¡å‹
```

---

### `generate_with_model()`

ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”Ÿæˆæ–‡æœ¬å†…å®¹ã€‚

```python
async def generate_with_model(
    prompt: str,
    model_config: TaskConfig,
    request_type: str = "plugin.generate",
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> Tuple[bool, str, str, str]
```

**å‚æ•°ï¼š**

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `prompt` | `str` | æç¤ºè¯ |
| `model_config` | `TaskConfig` | æ¨¡å‹é…ç½®ï¼ˆä» `get_available_models()` è·å–ï¼‰ |
| `request_type` | `str` | è¯·æ±‚ç±»å‹æ ‡è¯†ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰ |
| `temperature` | `float \| None` | æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶éšæœºæ€§ï¼Œ0-2ï¼‰ |
| `max_tokens` | `int \| None` | æœ€å¤§ token æ•° |

**è¿”å›å€¼ï¼š** `(æˆåŠŸ, ç”Ÿæˆå†…å®¹, æ¨ç†è¿‡ç¨‹, æ¨¡å‹åç§°)`

**ç¤ºä¾‹ï¼š**

```python
from src.plugin_system import llm_api

# è·å–å¯ç”¨æ¨¡å‹
models = llm_api.get_available_models()
model = models.get("utils")  # æˆ–å…¶ä»–æ¨¡å‹å

if model:
    success, content, reasoning, model_name = await llm_api.generate_with_model(
        prompt="è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±",
        model_config=model,
    )
    if success:
        await self.send_text(content)
```

---

### `generate_with_model_with_tools()`

ä½¿ç”¨æ¨¡å‹å’Œå·¥å…·è°ƒç”¨ç”Ÿæˆå†…å®¹ï¼ˆæ”¯æŒ Function Callingï¼‰ã€‚

```python
async def generate_with_model_with_tools(
    prompt: str,
    model_config: TaskConfig,
    tool_options: List[Dict[str, Any]] | None = None,
    request_type: str = "plugin.generate",
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> Tuple[bool, str, str, str, List[ToolCall] | None]
```

**è¿”å›å€¼ï¼š** `(æˆåŠŸ, ç”Ÿæˆå†…å®¹, æ¨ç†è¿‡ç¨‹, æ¨¡å‹åç§°, å·¥å…·è°ƒç”¨åˆ—è¡¨)`

**ç¤ºä¾‹ï¼š**

```python
# å®šä¹‰å·¥å…·
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°"
                    }
                },
                "required": ["city"]
            }
        }
    }
]

models = llm_api.get_available_models()
model = models.get("utils")

success, content, reasoning, model_name, tool_calls = \
    await llm_api.generate_with_model_with_tools(
        prompt="ä¸Šæµ·ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        model_config=model,
        tool_options=tools,
    )

if tool_calls:
    for call in tool_calls:
        print(f"å·¥å…·è°ƒç”¨ï¼š{call.function.name}({call.function.arguments})")
```

---

## å®ç”¨ç¤ºä¾‹

### ç¿»è¯‘åŠŸèƒ½

```python
async def execute(self):
    text = self.action_data.get("text", "")
    
    models = llm_api.get_available_models()
    model = models.get("utils")
    if not model:
        await self.send_text("âŒ æ¨¡å‹ä¸å¯ç”¨")
        return False, "æ¨¡å‹ä¸å¯ç”¨"
    
    success, result, _, _ = await llm_api.generate_with_model(
        prompt=f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼š\n{text}",
        model_config=model,
    )
    
    if success:
        await self.send_text(f"ç¿»è¯‘ç»“æœï¼š{result}")
    return success, "ç¿»è¯‘å®Œæˆ"
```

### å†…å®¹å®¡æ ¸

```python
async def execute(self):
    message = self.action_data.get("message", "")
    
    models = llm_api.get_available_models()
    model = models.get("utils")
    
    success, result, _, _ = await llm_api.generate_with_model(
        prompt=f"åˆ¤æ–­ä»¥ä¸‹å†…å®¹æ˜¯å¦å«æœ‰ä¸å½“ä¿¡æ¯ï¼Œåªå›ç­”'æ˜¯'æˆ–'å¦'ï¼š\n{message}",
        model_config=model,
        temperature=0.1,  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§çš„å›ç­”
    )
    
    is_inappropriate = success and "æ˜¯" in result
    return True, f"å®¡æ ¸å®Œæˆï¼š{'ä¸å½“' if is_inappropriate else 'æ­£å¸¸'}"
```

::: tip ä½•æ—¶ä½¿ç”¨ LLM API vs ç”Ÿæˆå™¨ APIï¼Ÿ
- **LLM API**ï¼šç›´æ¥è°ƒç”¨æ¨¡å‹ï¼Œå®Œå…¨æ§åˆ¶ promptï¼Œé€‚åˆç¿»è¯‘ã€åˆ†æã€åˆ¤æ–­ç­‰ç»“æ„åŒ–ä»»åŠ¡
- **ç”Ÿæˆå™¨ API**ï¼šä½¿ç”¨éº¦éº¦çš„é£æ ¼åŒ–ç”Ÿæˆå™¨ï¼Œå›å¤æ›´æ‹ŸäººåŒ–ï¼Œé€‚åˆéº¦éº¦å¯¹è¯å›å¤
:::
