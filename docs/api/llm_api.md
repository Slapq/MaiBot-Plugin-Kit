# ğŸ¤– LLM API

> **æ¥æº**ï¼š`src.plugin_system.apis.llm_api`

LLM API æä¾›ç›´æ¥è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼ˆä¸èµ°éº¦éº¦çš„å›å¤ç”Ÿæˆå™¨ï¼Œç›´æ¥è£¸è°ƒï¼‰ã€‚

## å¯¼å…¥æ–¹å¼

```python
from src.plugin_system import llm_api
# æˆ–
from src.plugin_system.apis import llm_api
```

---

## ä¸»è¦åŠŸèƒ½

### 1. æŸ¥è¯¢å¯ç”¨æ¨¡å‹

```python
models = llm_api.get_available_models()
# è¿”å›ï¼šDict[str, TaskConfig]
# key ä¸ºæ¨¡å‹åç§°ï¼Œvalue ä¸º TaskConfig å¯¹è±¡
```

### 2. ç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹

```python
success, content, reasoning, model_name = await llm_api.generate_with_model(
    prompt="ä½ çš„æç¤ºè¯",
    model_config=models["model_name"],   # ä» get_available_models() è·å–
    request_type="plugin.generate",      # å¯é€‰ï¼Œç”¨äºæ—¥å¿—è®°å½•
    temperature=0.8,                     # å¯é€‰ï¼Œå½±å“éšæœºæ€§ï¼ˆ0~2ï¼‰
    max_tokens=500,                      # å¯é€‰ï¼Œæœ€å¤§ç”Ÿæˆ token æ•°
)
# è¿”å›ï¼šTuple[bool, str, str, str]
# â†’ (æ˜¯å¦æˆåŠŸ, ç”Ÿæˆå†…å®¹, æ¨ç†è¿‡ç¨‹, å®é™…ä½¿ç”¨çš„æ¨¡å‹å)
```

### 3. å¸¦ Tool çš„ç”Ÿæˆ

```python
from src.plugin_system import tool_api

tools = tool_api.get_llm_available_tool_definitions()

success, content, reasoning, model_name, tool_calls = await llm_api.generate_with_model_with_tools(
    prompt="ä½ çš„æç¤ºè¯",
    model_config=models["model_name"],
    tool_options=tools,                  # ä¼ å…¥å·¥å…·åˆ—è¡¨
    request_type="plugin.generate",
    temperature=0.8,
    max_tokens=500,
)
# è¿”å›ï¼šTuple[bool, str, str, str, List[ToolCall] | None]
```

---

## å®Œæ•´ç¤ºä¾‹

```python
from src.plugin_system import (
    BaseCommand, ComponentInfo, ConfigField,
    llm_api,
)
from src.common.logger import get_logger

logger = get_logger("my_llm_plugin")


class AskCommand(BaseCommand):
    command_name = "ask"
    command_description = "å‘ AI æé—®"
    command_pattern = r"^/ask\s+(?P<question>.+)$"

    async def execute(self):
        question = self.matched_groups.get("question", "")
        if not question:
            await self.send_text("è¯·è¾“å…¥é—®é¢˜ï¼Œä¾‹å¦‚ï¼š/ask ä»€ä¹ˆæ˜¯é»‘æ´ï¼Ÿ")
            return True, "æ— é—®é¢˜", True

        # è·å–å¯ç”¨æ¨¡å‹
        models = llm_api.get_available_models()
        if not models:
            await self.send_text("æš‚æ— å¯ç”¨æ¨¡å‹")
            return False, "æ— æ¨¡å‹", True

        model_config = list(models.values())[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹

        # è°ƒç”¨ LLM
        success, content, reasoning, model_name = await llm_api.generate_with_model(
            prompt=f"è¯·ç®€æ´å›ç­”ï¼š{question}",
            model_config=model_config,
            request_type="plugin.ask",
            temperature=0.7,
            max_tokens=300,
        )

        if success and content:
            await self.send_text(content)
            logger.info(f"[ask] ä½¿ç”¨æ¨¡å‹ {model_name} å›ç­”äº†ï¼š{question}")
        else:
            await self.send_text("æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”å¤±è´¥äº† ğŸ˜…")

        return True, f"å›ç­”äº†é—®é¢˜ï¼š{question}", True
```

---

## æ³¨æ„äº‹é¡¹

- æ¯æ¬¡è°ƒç”¨éƒ½ä¼šæ¶ˆè€— API Tokenï¼Œæ³¨æ„æ§åˆ¶é¢‘ç‡
- `request_type` ç”¨äºæ—¥å¿—åˆ†æï¼Œå»ºè®®å¡«å†™æœ‰æ„ä¹‰çš„å­—ç¬¦ä¸²
- æ¨¡å‹åˆ—è¡¨ç”± MaiBot é…ç½®æ–‡ä»¶å†³å®šï¼Œæ’ä»¶æ— æ³•ç›´æ¥æŒ‡å®šæ¨¡å‹åç§°ï¼ˆéœ€ä» `get_available_models()` è·å–ï¼‰
- ä¸ `generator_api` çš„åŒºåˆ«ï¼š`llm_api` æ˜¯è£¸è°ƒ LLMï¼Œä¸è€ƒè™‘ä¸Šä¸‹æ–‡ï¼›`generator_api` æ˜¯å®Œæ•´çš„å›å¤ç”Ÿæˆæµç¨‹ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ã€äººè®¾ç­‰
